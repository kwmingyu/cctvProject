import sys  # íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„° ë° ì‹œìŠ¤í…œ í•¨ìˆ˜ ì œì–´ë¥¼ ìœ„í•œ ëª¨ë“ˆ
import cv2  # OpenCV: ì˜ìƒ ì²˜ë¦¬ ë° ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬
import datetime  # ë‚ ì§œ ë° ì‹œê°„ ì²˜ë¦¬ ëª¨ë“ˆ
import os  # ìš´ì˜ì²´ì œ ê²½ë¡œ ë° íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ
import time  # ì‹œê°„ ì§€ì—° ë° íƒ€ì´ë° í•¨ìˆ˜ ì œê³µ ëª¨ë“ˆ
import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°ì„ ìœ„í•œ NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch  # PyTorch: ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜ ë° ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import joblib  # ëª¨ë¸ ë° ì „ì²˜ë¦¬ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥/ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch import nn  # ì‹ ê²½ë§ ë ˆì´ì–´ ë° ëª¨ë¸ êµ¬ì„± ìš”ì†Œ
from collections import defaultdict, deque  # í‚¤ ê¸°ë³¸ê°’ ë”•ì…”ë„ˆë¦¬ì™€ ê³ ì • ê¸¸ì´ í ìë£Œêµ¬ì¡°
from PyQt5.QtWidgets import (
    QApplication, QLabel, QPushButton, QVBoxLayout, QHBoxLayout, QWidget,
    QComboBox, QTextEdit, QFileDialog, QLineEdit, QStatusBar, QMessageBox, QCheckBox
)  # PyQt5 GUI êµ¬ì„± ìš”ì†Œë“¤
from PyQt5.QtGui import QImage, QPixmap, QIcon  # ì´ë¯¸ì§€ í‘œí˜„ ë° ì•„ì´ì½˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
from PyQt5.QtCore import QTimer, Qt  # íƒ€ì´ë¨¸ ë° ì •ë ¬/ì •ë ¬ ìƒìˆ˜
from ultralytics import YOLO  # Ultralytics YOLOv8 ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡


# === ë‹¨ìˆœ ì¶”ì ê¸° í´ë˜ìŠ¤ ì •ì˜ ===
class SimpleTracker:
    def __init__(self, max_age=30):
        # next_id: ìƒˆ ê°ì²´ì— ë¶€ì—¬í•  ì¶”ì  ID
        # tracks: í˜„ì¬ í”„ë ˆì„ì—ì„œ íƒì§€ëœ ê°ì²´ë“¤ì˜ {ID: (x, y)} ì €ì¥
        # last_seen: ê° IDê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°±ì‹ ëœ ì´í›„ ì§€ë‚œ í”„ë ˆì„ ìˆ˜ ì¹´ìš´íŠ¸
        # max_age: ì§€ì • í”„ë ˆì„ ì´ìƒ ê°±ì‹  ì—†ìœ¼ë©´ ì¶”ì  ëŒ€ìƒì—ì„œ ì œê±°
        self.next_id = 1
        self.tracks = {}
        self.last_seen = {}
        self.max_age = max_age

    def update(self, detections):
        """
        detections: ë¦¬ìŠ¤íŠ¸ of (bbox, confidence, class)
        bbox: [x1, y1, x2, y2]
        ë°˜í™˜: í˜„ì¬ ì—…ë°ì´íŠ¸ëœ ì¶”ì  ê°ì²´ ë¦¬ìŠ¤íŠ¸ [(id, (cx, cy)), ...]
        """
        updated_tracks = {}
        # ê° íƒì§€ ê²°ê³¼ ìˆœíšŒ
        for det in detections:
            bbox, conf, cls = det
            # bbox ì¤‘ì‹¬ì  ê³„ì‚°
            cx, cy = (bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2
            matched = False
            # ê¸°ì¡´ ì¶”ì  ëŒ€ìƒê³¼ ê±°ë¦¬ ë¹„êµ
            for tid, (px, py) in self.tracks.items():
                # ì¤‘ì‹¬ì  ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
                if np.linalg.norm([cx - px, cy - py]) < 50:
                    # ê°™ì€ ê°ì²´ë¡œ íŒë‹¨í•˜ì—¬ ê¸°ì¡´ ID ì‚¬ìš©
                    updated_tracks[tid] = (cx, cy)
                    # ë§ˆì§€ë§‰ ê°±ì‹  í”„ë ˆì„ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                    self.last_seen[tid] = 0
                    matched = True
                    break
            if not matched:
                # ìƒˆë¡œìš´ ê°ì²´ë¼ë©´ ìƒˆë¡œìš´ ID ë¶€ì—¬
                updated_tracks[self.next_id] = (cx, cy)
                self.last_seen[self.next_id] = 0
                self.next_id += 1

        # ê°±ì‹ ë˜ì§€ ì•Šì€ IDë“¤ì— ëŒ€í•´ age ì¦ê°€ì‹œí‚¤ê³ , max_age ì´ˆê³¼ ì‹œ ì œê±°
        to_delete = []
        for tid in self.last_seen:
            if tid not in updated_tracks:
                self.last_seen[tid] += 1
                if self.last_seen[tid] > self.max_age:
                    to_delete.append(tid)
        for tid in to_delete:
            updated_tracks.pop(tid, None)
            self.last_seen.pop(tid, None)

        # ìµœì¢… íŠ¸ë™ ê°±ì‹ 
        self.tracks = updated_tracks
        # IDì™€ ì¤‘ì‹¬ì  ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return list(self.tracks.items())


# === LSTM AutoEncoder ì‹ ê²½ë§ ì •ì˜ ===
class LSTMAutoEncoder(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=16):
        super().__init__()
        # ì¸ì½”ë”: input_dim ì°¨ì› ì…ë ¥ì„ hidden_dim ì°¨ì›ìœ¼ë¡œ ì••ì¶•
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        # ë””ì½”ë”: hidden_dim ì°¨ì› ì •ë³´ë¥¼ ë‹¤ì‹œ input_dim ì°¨ì›ìœ¼ë¡œ ë³µì›
        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)

    def forward(self, x):
        # ì¸ì½”ë” í†µê³¼ í›„ íˆë“  ìƒíƒœ h íšë“
        _, (h, _) = self.encoder(x)
        # íˆë“  ìƒíƒœë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µí•˜ì—¬ ë””ì½”ë” ì…ë ¥ ìƒì„±
        repeated = h.repeat(x.size(1), 1, 1).permute(1, 0, 2)
        # ë””ì½”ë”ë¥¼ í†µí•´ ë³µì›ëœ ì‹œí€€ìŠ¤ out ë°˜í™˜
        out, _ = self.decoder(repeated)
        return out


# === í´ëŸ¬ìŠ¤í„°ë§ ë° í•„í„°ë§ í•¨ìˆ˜ ì •ì˜ ===
def cluster_and_filter(boxes, classes, confs, threshold=80):
    """
    boxes: Nx4 array of [x1,y1,x2,y2]
    classes: N array of class IDs
    confs: N array of confidences
    threshold: í”½ì…€ ê±°ë¦¬ ì„ê³„ê°’
    ê°™ì€ í´ë˜ìŠ¤(ì£¼ì°¨ëœ ì°¨ëŸ‰ ë“±) ì¤‘ì‹¬ì  ê°„ ê±°ë¦¬ê°€ threshold ì´í•˜ì¸ ë°•ìŠ¤ë“¤ êµ°ì§‘í™”
    êµ°ì§‘ë§ˆë‹¤ confidence ìµœëŒ€ê°’ì„ ê°€ì§„ ë°•ìŠ¤ë§Œ ì„ íƒí•˜ì—¬ ë°˜í™˜
    """
    clusters = []
    used = set()
    for i in range(len(boxes)):
        if i in used:
            continue
        cls_i = int(classes[i])
        # ì›í•˜ëŠ” í´ë˜ìŠ¤(0ë²ˆ: ì‚¬ëŒ ë“±)ë§Œ ì²˜ë¦¬
        if cls_i != 0:
            continue
        x1, y1, x2, y2 = boxes[i]
        cx_i, cy_i = (x1 + x2) / 2, (y1 + y2) / 2
        cluster = [(i, confs[i], boxes[i])]
        used.add(i)
        # ì´í›„ ë°•ìŠ¤ë“¤ë„ ê°™ì€ í´ë˜ìŠ¤ì´ë©´ì„œ threshold ì´ë‚´ì— ìˆìœ¼ë©´ êµ°ì§‘ì— ì¶”ê°€
        for j in range(i + 1, len(boxes)):
            if j in used:
                continue
            cls_j = int(classes[j])
            if cls_j != cls_i:
                continue
            x1j, y1j, x2j, y2j = boxes[j]
            cx_j, cy_j = (x1j + x2j) / 2, (y1j + y2j) / 2
            if np.linalg.norm([cx_i - cx_j, cy_i - cy_j]) < threshold:
                cluster.append((j, confs[j], boxes[j]))
                used.add(j)
        # êµ°ì§‘ ì¤‘ confidence ìµœëŒ€ê°’ì„ ê°€ì§„ ë°•ìŠ¤ë¥¼ ì„ íƒ
        best = max(cluster, key=lambda x: x[1])
        clusters.append(best)
    return clusters


# === PyQt5 ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ CCTV ì• í”Œë¦¬ì¼€ì´ì…˜ GUI í´ë˜ìŠ¤ ì •ì˜ ===
class CCTVApp(QWidget):
    def __init__(self):
        super().__init__()
        # ìœˆë„ìš° íƒ€ì´í‹€ ë° í¬ê¸°/ìŠ¤íƒ€ì¼ ì„¤ì •
        self.setWindowTitle("ğŸ›¡ï¸ ìŠ¤ë§ˆíŠ¸ CCTV ê´€ì œ ì‹œìŠ¤í…œ")
        self.setGeometry(200, 100, 1100, 700)
        self.setStyleSheet("background-color: #121212; color: #e0e0e0; font-family: 'Segoe UI';")

        # ì˜ìƒ í‘œì‹œ í¬ê¸° ì§€ì •
        self.video_width, self.video_height = 1440, 960
        self.video_path = None  # íŒŒì¼ ì„ íƒ ëª¨ë“œë¥¼ ìœ„í•œ ê²½ë¡œ ë³€ìˆ˜

        # ì˜ìƒ ë ˆì´ë¸”: í”„ë ˆì„ì„ ë³´ì—¬ì¤„ QLabel
        self.video_label = QLabel()
        self.video_label.setFixedSize(self.video_width, self.video_height)
        self.video_label.setStyleSheet("background-color: #000; border: 2px solid #2e86de; border-radius: 10px;")
        self.video_label.setAlignment(Qt.AlignCenter)

        # ì¹´ë©”ë¼/íŒŒì¼ ì„ íƒ ì½¤ë³´ë°•ìŠ¤
        self.camera_select = QComboBox()
        self.camera_select.addItems(["ì¹´ë©”ë¼ 0", "ì¹´ë©”ë¼ 1", "RTSP ì§ì ‘ ì…ë ¥", "ğŸ ì˜ìƒ íŒŒì¼"])
        self.camera_select.currentIndexChanged.connect(self.toggle_rtsp_input)

        # RTSP URL ì…ë ¥ì°½
        self.rtsp_input = QLineEdit()
        self.rtsp_input.setPlaceholderText("RTSP ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        self.rtsp_input.setEnabled(False)  # ê¸°ë³¸ ë¹„í™œì„±í™”

        # íŒŒì¼ ì„ íƒ ë²„íŠ¼
        self.video_select_btn = QPushButton("ğŸ ì˜ìƒ íŒŒì¼ ì„ íƒ")
        self.video_select_btn.setStyleSheet(
            "QPushButton { background-color: #f39c12; border-radius: 8px; padding: 10px; "
            "color: white; font-weight: bold; } "
            "QPushButton:hover { background-color: #d35400; }"
        )

        self.video_select_btn.clicked.connect(self.select_video_file)

        # íšŒì „ ì²´í¬ë°•ìŠ¤
        self.rotate_checkbox = QCheckBox("ğŸŒ€ ì˜ìƒ ì™¼ìª½ìœ¼ë¡œ íšŒì „")
        self.rotate_checkbox.setStyleSheet("color: white; font-weight: bold; padding: 5px;")

        # ì¬ìƒ/ì •ì§€/ë…¹í™”/ìŠ¤ëƒ…ìƒ· ë“± ì»¨íŠ¸ë¡¤ ë²„íŠ¼ ìŠ¤íƒ€ì¼ ì •ì˜
        btn_style = "QPushButton { background-color: #3498db; border-radius: 8px; padding: 10px; color: white; font-weight: bold; } QPushButton:hover { background-color: #2980b9; }"
        self.start_btn = QPushButton("â–¶ ì¬ìƒ"); self.start_btn.setStyleSheet(btn_style)
        self.stop_btn = QPushButton("â¹ ì •ì§€"); self.stop_btn.setStyleSheet(btn_style)
        self.record_btn = QPushButton("âº ë…¹í™”"); self.record_btn.setStyleSheet(btn_style)
        self.snapshot_btn = QPushButton("ğŸ“¸ ìŠ¤ëƒ…ìƒ·"); self.snapshot_btn.setStyleSheet(btn_style)
        self.select_path_btn = QPushButton("ğŸ“ ì €ì¥ ê²½ë¡œ ì„¤ì •"); self.select_path_btn.setStyleSheet(btn_style)
        self.save_log_btn = QPushButton("ğŸ§¾ ë¡œê·¸ ì €ì¥"); self.save_log_btn.setStyleSheet(btn_style)

        # ì €ì¥ ê²½ë¡œ í‘œì‹œ ë¼ë²¨ ë° ë¡œê·¸ í…ìŠ¤íŠ¸ ì—ë””íŠ¸
        self.path_label = QLabel(f"ì €ì¥ ê²½ë¡œ: {os.getcwd()}")
        self.log_text = QTextEdit(); self.log_text.setReadOnly(True); self.log_text.setFixedHeight(580)
        self.status_bar = QStatusBar()

        # ë ˆì´ì•„ì›ƒ ë°°ì¹˜: ì¹´ë©”ë¼ ì„ íƒ + RTSP ì…ë ¥ + íŒŒì¼ ì„ íƒ ë²„íŠ¼
        cam_layout = QHBoxLayout()
        cam_layout.addWidget(self.camera_select)
        cam_layout.addWidget(self.rtsp_input)
        cam_layout.addWidget(self.video_select_btn)

        # ì œì–´ ë²„íŠ¼ ë ˆì´ì•„ì›ƒ
        control = QVBoxLayout()
        control.addLayout(cam_layout)
        control.addWidget(self.rotate_checkbox)
        for b in [self.start_btn, self.stop_btn, self.record_btn, self.snapshot_btn,
                  self.select_path_btn, self.save_log_btn, self.path_label, self.log_text, self.status_bar]:
            control.addWidget(b)

        # ë©”ì¸ ë ˆì´ì•„ì›ƒ: ì˜ìƒ ë ˆì´ë¸” + ì»¨íŠ¸ë¡¤
        layout = QHBoxLayout(); layout.addWidget(self.video_label); layout.addLayout(control)
        self.setLayout(layout)

        # ìº¡ì²˜ ë° ê¸°ë¡ ì´ˆê¸°ê°’ ì„¤ì •
        self.cap = None  # OpenCV VideoCapture ê°ì²´
        self.writer = None  # ë¹„ë””ì˜¤ ê¸°ë¡ì„ ìœ„í•œ VideoWriter
        self.save_path = os.getcwd()  # ë…¹í™” ë° ìŠ¤ëƒ…ìƒ· ì €ì¥ ê²½ë¡œ
        self.is_recording = False  # ë…¹í™” ìƒíƒœ í”Œë˜ê·¸

        # ë²„íŠ¼ í´ë¦­ ì‹œ ì—°ê²°í•  ë©”ì„œë“œ ë“±ë¡
        self.start_btn.clicked.connect(self.start_camera)
        self.stop_btn.clicked.connect(self.stop_camera)
        self.record_btn.clicked.connect(self.toggle_recording)
        self.snapshot_btn.clicked.connect(self.save_snapshot)
        self.select_path_btn.clicked.connect(self.select_save_path)
        self.save_log_btn.clicked.connect(self.save_log)

        # íƒ€ì´ë¨¸: ì•½ 33ms ê°„ê²©ìœ¼ë¡œ update_frame í˜¸ì¶œ (ì•½ 30fps)
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •: GPU(cuda) ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPU ì‚¬ìš©
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        self.model = LSTMAutoEncoder().to(self.device)
        model_path = os.path.join(os.path.dirname(__file__), "ae_model.pt")
        self.scaler = joblib.load("scaler.pkl")  # ê±°ë¦¬ ë° ì†ë„ íŠ¹ì„± ìŠ¤ì¼€ì¼ëŸ¬
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜

        # YOLO ê°ì²´ íƒì§€ ëª¨ë¸ ë¡œë“œ
        self.yolo = YOLO("yolov8n.pt")
        # ë‹¨ìˆœ ì¶”ì ê¸°, ê¶¤ì  ë²„í¼, persistence ì¹´ìš´í„° ì´ˆê¸°í™”
        self.tracker = SimpleTracker()
        self.track_buffers = defaultdict(lambda: deque(maxlen=30))
        self.persistence = defaultdict(int)
        # ì¬êµ¬ì„± ì˜¤ì°¨ ìŠ¤ì½”ì–´ ì„ê³„ê°’
        self.MIN_SCORE = 0.0003
        self.MAX_SCORE = 0.0070

    def toggle_rtsp_input(self, index):
        """ì½¤ë³´ë°•ìŠ¤ ì¸ë±ìŠ¤ ë³€ê²½ ì‹œ RTSP ì…ë ¥ì°½ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.rtsp_input.setEnabled(index == 2)
        self.video_select_btn.setEnabled(index == 3)

    def select_video_file(self):
        """íŒŒì¼ ëŒ€í™”ìƒìë¥¼ ì—´ì–´ ì˜ìƒ íŒŒì¼ ì„ íƒ"""
        path, _ = QFileDialog.getOpenFileName(self, "ì˜ìƒ íŒŒì¼ ì„ íƒ", "", "Video Files (*.mp4 *.avi *.mov)")
        if path:
            self.video_path = path
            self.log(f"ì˜ìƒ íŒŒì¼ ì„ íƒë¨: {path}")

    def start_camera(self):
        """ì¹´ë©”ë¼ ë˜ëŠ” íŒŒì¼/RTSP ì¬ìƒ ì‹œì‘"""
        index = self.camera_select.currentIndex()
        if index == 2:
            src = self.rtsp_input.text().strip()
        elif index == 3:
            if not self.video_path:
                QMessageBox.warning(self, "ê²½ê³ ", "ì˜ìƒ íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            src = self.video_path
        else:
            src = index

        self.cap = cv2.VideoCapture(src)
        if not self.cap.isOpened():
            QMessageBox.critical(self, "ì˜¤ë¥˜", "ì¹´ë©”ë¼/ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨")
            return
        self.timer.start(33)  # 30fps ì¬ìƒ
        self.log("ì¹´ë©”ë¼/ì˜ìƒ ì¬ìƒ ì‹œì‘ë¨")

    def stop_camera(self):
        """ì¬ìƒ ë° ë…¹í™” ì¤‘ì§€"""
        self.timer.stop()
        if self.cap:
            self.cap.release()
            self.cap = None
        if self.writer:
            self.writer.release()
            self.writer = None
        self.video_label.clear()
        self.log("ì¬ìƒ ì •ì§€ë¨")

    def update_frame(self):
        """íƒ€ì´ë¨¸ í˜¸ì¶œë¡œ í”„ë ˆì„ ê°±ì‹  ë° ê°ì²´ íƒì§€/ì¶”ì /ì´ìƒ í–‰ë™ íŒë³„"""
        if not self.cap:
            return
        ret, frame = self.cap.read()
        if not ret:
            return

        # íšŒì „ ì˜µì…˜ ì ìš©
        if self.rotate_checkbox.isChecked():
            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)

        # YOLO ê°ì²´ íƒì§€ ìˆ˜í–‰
        results = self.yolo.predict(frame, conf=0.5)[0]
        boxes = results.boxes.xyxy.cpu().numpy()
        classes = results.boxes.cls.cpu().numpy()
        confs = results.boxes.conf.cpu().numpy()

        # í´ëŸ¬ìŠ¤í„°ë§ ë° í•„í„°ë§ í›„ ì í•©í•œ íƒì§€ë§Œ ì„ íƒ
        clustered = cluster_and_filter(boxes, classes, confs)
        filtered = [(box, conf, int(classes[idx])) for idx, conf, box in clustered]
        input_dets = [([int(x1), int(y1), int(x2), int(y2)], conf, cls) for (x1, y1, x2, y2), conf, cls in filtered]

        # ì¶”ì  ì—…ë°ì´íŠ¸
        tracks = self.tracker.update(input_dets)

        # ê° ê°ì²´ì— ëŒ€í•´ ì†ë„, ê±°ë¦¬ ê³„ì‚° ë° ì´ìƒ í–‰ë™ íŒë³„ ë¡œì§
        for tid, (cx, cy) in tracks:
            buf = self.track_buffers[tid]
            if buf:
                px, py, _, _ = buf[-1]
                dist = np.hypot(cx - px, cy - py)
                speed = dist / 0.033  # 33ms ê¸°ì¤€
            else:
                dist = speed = 0
            buf.append((cx, cy, dist, speed))

            label = "Normal"
            color = (0, 255, 0)

            # ì¶©ë¶„í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ìŒ“ì´ë©´ ì´ìƒ í–‰ë™ í‰ê°€
            if len(buf) >= 30:
                seq = np.array([[b[2], b[3]] for b in buf], dtype=np.float32)
                seq_scaled = self.scaler.transform(seq)
                x = torch.tensor(seq_scaled.reshape(1, 30, 2), dtype=torch.float32).to(self.device)

                dist_seq = seq_scaled[:, 0]
                speed_seq = seq_scaled[:, 1]

                avg_speed = np.mean(speed_seq)
                total_dist = np.sum(dist_seq)
                high_speed_count = np.sum(speed_seq > 0.3)

                # ëŠë¦¬ê²Œ ì›€ì§ì¼ ê²½ìš° ë°°íšŒ(loitering)ë¡œ íŒë‹¨
                if avg_speed < 0.1 and total_dist < 0.2:
                    label = "Loitering"
                    color = (0, 165, 255)
                # ë¹ ë¥´ê²Œ ì›€ì§ì¼ ê²½ìš° ë‹¬ë¦¬ê¸°(running)ë¡œ íŒë‹¨
                elif high_speed_count >= 2:
                    label = "Running"
                    color = (255, 100, 0)

                # AutoEncoder ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ë°˜ ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
                with torch.no_grad():
                    recon = self.model(x)
                    score = torch.mean((x - recon) ** 2).item()
                    scaled = 100 * (score - self.MIN_SCORE) / (self.MAX_SCORE - self.MIN_SCORE + 1e-6)
            else:
                scaled = 0

            # ì—°ì† ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì´ìƒ í–‰ë™(Anomaly) í‘œì‹œ
            self.persistence[tid] = self.persistence[tid] + 1 if scaled >= 80 else 0
            if self.persistence[tid] >= 3:
                label = "Anomaly"
                color = (0, 0, 255)

            # í”„ë ˆì„ì— ë ˆì´ë¸” ë° ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.putText(frame, f"{label} ({scaled:.1f})", (cx - 50, cy - 20), 0, 0.6, color, 2)
            cv2.rectangle(frame, (cx - 30, cy - 60), (cx + 30, cy + 60), color, 2)

        # ë…¹í™” ì¤‘ì¼ ê²½ìš° ì˜ìƒ ê¸°ë¡
        if self.is_recording and self.writer:
            self.writer.write(frame)

        # OpenCV BGR -> RGB ë³€í™˜ í›„ QImageë¡œ ë³€í™˜í•˜ì—¬ QLabelì— í‘œì‹œ
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        img = QImage(rgb.data, w, h, ch * w, QImage.Format_RGB888)
        self.video_label.setPixmap(QPixmap.fromImage(img).scaled(
            self.video_label.width(),
            self.video_label.height(),
            Qt.KeepAspectRatio,
            Qt.SmoothTransformation
        ))

    def toggle_recording(self):
        """ë…¹í™” ì‹œì‘/ì¤‘ì§€ í† ê¸€"""
        if not self.cap:
            return
        if not self.is_recording:
            # íŒŒì¼ëª…ì— í˜„ì¬ ì‹œê° í¬í•¨
            now = datetime.datetime.now().strftime("record_%Y%m%d_%H%M%S.avi")
            path = os.path.join(self.save_path, now)
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            self.writer = cv2.VideoWriter(path, fourcc, 20.0, (w, h))
            self.is_recording = True
            self.log(f"ë…¹í™” ì‹œì‘: {path}")
            self.status_bar.showMessage("ë…¹í™” ì¤‘...")
        else:
            self.is_recording = False
            if self.writer:
                self.writer.release()
                self.writer = None
            self.log("ë…¹í™” ì¤‘ì§€ë¨")
            self.status_bar.showMessage("ë…¹í™” ì¤‘ì§€ë¨")

    def save_snapshot(self):
        """í˜„ì¬ í”„ë ˆì„ ìŠ¤ëƒ…ìƒ· ì €ì¥"""
        if not self.cap:
            return
        ret, frame = self.cap.read()
        if not ret:
            return
        name = datetime.datetime.now().strftime("snapshot_%Y%m%d_%H%M%S.jpg")
        path = os.path.join(self.save_path, name)
        cv2.imwrite(path, frame)
        self.log(f"ìŠ¤ëƒ…ìƒ· ì €ì¥: {path}")
        self.status_bar.showMessage("ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ", 3000)

    def select_save_path(self):
        """ì €ì¥ ê²½ë¡œ ë³€ê²½ ë‹¤ì´ì–¼ë¡œê·¸"""
        path = QFileDialog.getExistingDirectory(self, "ì €ì¥ ê²½ë¡œ ì„ íƒ", self.save_path)
        if path:
            self.save_path = path
            self.path_label.setText(f"ì €ì¥ ê²½ë¡œ: {self.save_path}")
            self.log(f"ì €ì¥ ê²½ë¡œ ë³€ê²½ë¨: {self.save_path}")

    def save_log(self):
        """ë¡œê·¸ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        path, _ = QFileDialog.getSaveFileName(self, "ë¡œê·¸ ì €ì¥", "log.txt", "Text Files (*.txt)")
        if path:
            with open(path, "w", encoding="utf-8") as f:
                f.write(self.log_text.toPlainText())
            self.log(f"ë¡œê·¸ ì €ì¥ë¨: {path}")
            self.status_bar.showMessage("ë¡œê·¸ ì €ì¥ ì™„ë£Œ", 3000)

    def log(self, msg):
        """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ ì—ë””íŠ¸ì— ì¶”ê°€"""
        now = datetime.datetime.now().strftime("[%H:%M:%S]")
        self.log_text.append(f"{now} {msg}")


# === ë©”ì¸ í•¨ìˆ˜ ===
if __name__ == "__main__":
    app = QApplication(sys.argv)  # QApplication ì´ˆê¸°í™”
    win = CCTVApp()  # CCTV ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    win.show()  # ìœˆë„ìš° í‘œì‹œ
    sys.exit(app.exec_())  # ì´ë²¤íŠ¸ ë£¨í”„ ì‹œì‘ìœ¼ë¡œ í”„ë¡œê·¸ë¨ ì‹¤í–‰ 